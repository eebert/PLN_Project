{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b6e261a-e4f7-4902-932f-35741e59c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** Unir TFM ***\n",
    "# *** Valorizacion ***\n",
    "# Javier Ulloa ***\n",
    "# ***************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a699d1-4dc7-4ddb-bad2-9fa8cedf2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "from azure.storage.blob import BlobServiceClient, generate_blob_sas, BlobSasPermissions\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "\n",
    "\n",
    "##Codigo Adicional agregado\n",
    "import sys\n",
    "\n",
    "# ConfiguraciÃ³n del entorno del notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from config import PDF_DIRECTORY_BOW, CSV_DIRECTORY_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c94f5b-46b6-4916-a610-35c7ab3de547",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpais\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mano\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnombre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapellido\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Descarga el archivo PDF\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msas_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_directory, filename), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     84\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(response\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\urllib3\\response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\urllib3\\response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\urllib3\\response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32md:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\urllib3\\response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;66;03m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 463\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b)[:n]\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;66;03m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\http\\client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    502\u001b[0m         b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmemoryview\u001b[39m(b)[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength]\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n \u001b[38;5;129;01mand\u001b[39;00m b:\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1242\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1241\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\ssl.py:1100\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# descarga nltk\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"omw\", quiet=True)\n",
    "\n",
    "# Credenciales de acceso a SQL server\n",
    "account_name = \"account_name\"\n",
    "account_key = \"account_key\"\n",
    "container_name = \"discursos\"\n",
    "\n",
    "\n",
    "# Creacion de un cliente de interaccion con blob storage\n",
    "connect_str = (\n",
    "    \"DefaultEndpointsProtocol=https;AccountName=\"\n",
    "    + account_name\n",
    "    + \";AccountKey=\"\n",
    "    + account_key\n",
    "    + \";EndpointSuffix=core.windows.net\"\n",
    ")\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "\n",
    "# conexion del container\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# obtener una lista de contenedores\n",
    "blob_list = []\n",
    "for blob_i in container_client.list_blobs():\n",
    "    blob_list.append(blob_i.name)\n",
    "# print(blob_list)\n",
    "\n",
    "\n",
    "# Ruta del directorio donde se descargaron los archivos PDF\n",
    "pdf_directory = PDF_DIRECTORY_BOW\n",
    "\n",
    "sas_urls = []\n",
    "# generate a shared access signiture for files and load them into Python\n",
    "for blob_i in blob_list:\n",
    "    # generate a shared access signature for each blob file\n",
    "    sas_i = generate_blob_sas(\n",
    "        account_name=account_name,\n",
    "        container_name=container_name,\n",
    "        blob_name=blob_i,\n",
    "        account_key=account_key,\n",
    "        permission=BlobSasPermissions(read=True),\n",
    "        expiry=datetime.utcnow() + timedelta(hours=1),\n",
    "    )\n",
    "\n",
    "    sas_url = (\n",
    "        \"https://\"\n",
    "        + account_name\n",
    "        + \".blob.core.windows.net/\"\n",
    "        + container_name\n",
    "        + \"/\"\n",
    "        + blob_i\n",
    "        + \"?\"\n",
    "        + sas_i\n",
    "    )\n",
    "    sas_urls.append(sas_url)\n",
    "\n",
    "# print(sas_urls)\n",
    "\n",
    "# Lista de archivos PDF\n",
    "pdf_files = []\n",
    "\n",
    "# Extrae el paÃ­s, el presidente y el aÃ±o del nombre del archivo\n",
    "for sas_url in sas_urls:\n",
    "    nombre = sas_url.split(\".\")[4]\n",
    "    nombre_sin = nombre.split(\"/\")[-1]\n",
    "    nombre_archivo = nombre_sin.split(\".\")[0]\n",
    "    president = sas_url.split(\"/\")[-3]\n",
    "    country = sas_url.split(\"/\")[-4]\n",
    "    pais = nombre_sin.split(\"_\")[0]\n",
    "    ano = nombre_sin.split(\"_\")[1]\n",
    "    nombre = nombre_sin.split(\"_\")[2]\n",
    "    apellido = nombre_archivo.split(\"_\")[3]\n",
    "\n",
    "    # Genera el nombre del archivo PDF\n",
    "    filename = f\"{pais}_{ano}_{nombre}_{apellido}.pdf\"\n",
    "\n",
    "    # Descarga el archivo PDF\n",
    "    response = requests.get(sas_url)\n",
    "    with open(os.path.join(pdf_directory, filename), \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Agrega el archivo PDF a la lista de archivos descargados\n",
    "    pdf_files.append(filename)\n",
    "\n",
    "    year = \"\"\n",
    "    nombre_archivo = \"\"\n",
    "    president = \"\"\n",
    "    country = \"\"\n",
    "    pais = \"\"\n",
    "    ano = \"\"\n",
    "    nombre = \"\"\n",
    "    apellido = \"\"\n",
    "\n",
    "# Imprime la lista de archivos PDF\n",
    "# print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2517f8-1693-4a43-a10d-19d6c8e115f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1era Parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eb2f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"vader_lexicon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba539b3-1b47-48a2-982b-5d893d681639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# ##########################\n",
    "# Polarizacion\n",
    "# ##########################\n",
    "# Palabras mas frecuentes\n",
    "###########################\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "####################\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"treebank\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "\n",
    "# Archivos descargados de la nube\n",
    "carpeta_pdf = PDF_DIRECTORY_BOW\n",
    "####################\n",
    "\n",
    "# Descargar el modelo en espaÃ±ol para spaCy\n",
    "spacy.cli.download(\"es_core_news_sm\")\n",
    "\n",
    "# Inicializar el lematizador de NLTK\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Diccionario para almacenar palabras frecuentes por oraciÃ³n\n",
    "# sin stopwords\n",
    "palabras_frecuentes = {}\n",
    "# con stopwords\n",
    "palabras_frecuentes2 = {}\n",
    "\n",
    "\n",
    "# Recueprar texto de los PDFs\n",
    "def procesar_oraciones(ruta_pdf):\n",
    "    with open(ruta_pdf, \"rb\") as archivo_pdf:\n",
    "        cont_pdf = PdfReader(archivo_pdf)\n",
    "        texto = \"\"\n",
    "        for i in range(len(cont_pdf.pages)):\n",
    "            pagina = cont_pdf.pages[i]\n",
    "            texto += pagina.extract_text()\n",
    "        oraciones = sent_tokenize(texto)\n",
    "    return oraciones\n",
    "\n",
    "\n",
    "# Polarizacion de los textos\n",
    "def clasificar_texto(oracion):\n",
    "    termometro = SentimentIntensityAnalyzer()\n",
    "    temperatura = termometro.polarity_scores(oracion)[\"compound\"]\n",
    "    if temperatura >= 0.05:\n",
    "        resultado = \"Positivo\"\n",
    "    elif temperatura <= -0.05:\n",
    "        resultado = \"Negativo\"\n",
    "    else:\n",
    "        resultado = \"Neutral\"\n",
    "    return temperatura, resultado\n",
    "\n",
    "\n",
    "# Proceso de los discursos presidenciales\n",
    "def procesar_archivos_en_carpeta(carpeta_pdf):\n",
    "    campos = [\n",
    "        \"Fecha\",\n",
    "        \"Pais\",\n",
    "        \"Presidente\",\n",
    "        \"Orden\",\n",
    "        \"Discurso\",\n",
    "        \"Oracion\",\n",
    "        \"TextoProcesado\",\n",
    "        \"PalabrasEliminadas\",\n",
    "        \"Clasificacion\",\n",
    "        \"Valoracion\",\n",
    "        \"Clasificacion2\",\n",
    "        \"Valoracion2\",\n",
    "        \"PalabrasFrecuentes\",\n",
    "        \"PalabrasFrecuentes2\",\n",
    "    ]\n",
    "    data_list = []\n",
    "\n",
    "    for archivo_pdf in os.listdir(carpeta_pdf):\n",
    "        if archivo_pdf.endswith(\".pdf\"):\n",
    "            ruta_pdf = os.path.join(carpeta_pdf, archivo_pdf)\n",
    "            nombre_archivo = os.path.splitext(os.path.basename(ruta_pdf))[0]\n",
    "            elementos = nombre_archivo.split(\"_\")\n",
    "            pais = elementos[0]\n",
    "            anio = int(elementos[1])\n",
    "            nombre_presidente = \" \".join(elementos[2:])\n",
    "            oraciones = procesar_oraciones(ruta_pdf)\n",
    "\n",
    "            for i, oracion in enumerate(oraciones, start=1):\n",
    "                oracion_con_formato = oracion.replace(\"\\n\", \"\").replace(\"'\", \"\").strip()\n",
    "                # Calcula temperatura y valoraciÃ³n con la columna \"Oracion\"\n",
    "                valoracion, temperatura = clasificar_texto(oracion_con_formato)\n",
    "                stop_words = set(stopwords.words(\"spanish\"))\n",
    "                word_tokens = word_tokenize(oracion.lower())\n",
    "                filtered_sentence = [\n",
    "                    word\n",
    "                    for word in word_tokens\n",
    "                    if word.isalnum() and word not in stop_words\n",
    "                ]\n",
    "                texto_procesado = \" \".join(filtered_sentence)\n",
    "                # Palabras_a_eliminar\n",
    "                # palabras_a_eliminar = ['S','s', 'si', 'pr', 'Ã³n', 'Ã©s', 'c', 're', 'po', 'r', 'ta', 'pues','si', 'on', 'na', 'co', 're', 'blo','5','pues', 'e']\n",
    "\n",
    "                # Almacena las palabras eliminadas (stopwords)\n",
    "                palabras_eliminadas = [\n",
    "                    word for word in word_tokens if word.lower() in stop_words\n",
    "                ]\n",
    "                palabras_eliminadas_str = \"{\" + \", \".join(palabras_eliminadas) + \"}\"\n",
    "                # palabras_eliminadas_str = '{' + ', '.join(palabras_a_eliminar) + '}'\n",
    "                # Realiza el anÃ¡lisis de sentimientos para la columna \"TextoProcesado\"\n",
    "                valoracion2, temperatura2 = clasificar_texto(texto_procesado)\n",
    "                # LematizaciÃ³n e IdentificaciÃ³n de palabras frecuentes\n",
    "                nlp = spacy.load(\"es_core_news_sm\")\n",
    "                lematizados_spacy = [token.lemma_ for token in nlp(oracion_con_formato)]\n",
    "                lematizados_spacy2 = [token.lemma_ for token in nlp(texto_procesado)]\n",
    "\n",
    "                # Contar la frecuencia de cada palabra lematizada\n",
    "                frecuencia_palabras = Counter(lematizados_spacy)\n",
    "                frecuencia_palabras2 = Counter(lematizados_spacy2)\n",
    "\n",
    "                # Obtener las tres palabras mÃ¡s frecuentes\n",
    "                top_3_palabras = frecuencia_palabras.most_common(3)\n",
    "                top_3_palabras2 = frecuencia_palabras2.most_common(3)\n",
    "\n",
    "                palabras_frecuentes[oracion_con_formato] = [\n",
    "                    palabra for palabra, _ in top_3_palabras\n",
    "                ]\n",
    "                palabras_frecuentes2[texto_procesado] = [\n",
    "                    palabra for palabra, _ in top_3_palabras2\n",
    "                ]\n",
    "\n",
    "                valorizaciones = {\n",
    "                    \"Fecha\": anio,\n",
    "                    \"Pais\": pais,\n",
    "                    \"Presidente\": nombre_presidente,\n",
    "                    \"Orden\": i,\n",
    "                    \"Discurso\": nombre_archivo + \".pdf\",\n",
    "                    \"Oracion\": oracion_con_formato,\n",
    "                    \"TextoProcesado\": texto_procesado,\n",
    "                    \"PalabrasEliminadas\": palabras_eliminadas_str,\n",
    "                    \"Clasificacion\": temperatura,\n",
    "                    \"Valoracion\": valoracion,\n",
    "                    \"Clasificacion2\": temperatura2,\n",
    "                    \"Valoracion2\": valoracion2,\n",
    "                    \"PalabrasFrecuentes\": palabras_frecuentes[oracion_con_formato],\n",
    "                    \"PalabrasFrecuentes2\": palabras_frecuentes2[texto_procesado],\n",
    "                }\n",
    "\n",
    "                data_list.append(valorizaciones)\n",
    "\n",
    "    df = pd.DataFrame(data_list, columns=campos)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Procesa los archivos en la carpeta y crea el DataFrame\n",
    "data = procesar_archivos_en_carpeta(carpeta_pdf)\n",
    "\n",
    "# Guarda el DataFrame en un archivo CSV\n",
    "csv_output_path = CSV_DIRECTORY_BOW + \"\\speech_evaluation.csv\"  # ojo modifcicacion\n",
    "data.to_csv(csv_output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
