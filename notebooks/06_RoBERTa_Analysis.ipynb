{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 17:41:07,931 - INFO - loading weights file pytorch_model.bin from cache at C:\\Users\\User\\.cache\\huggingface\\hub\\models--PlanTL-GOB-ES--roberta-large-bne-capitel-ner\\snapshots\\87b8cbf77bf17db1e79441f5cb0c1a1ab4bf40bf\\pytorch_model.bin\n",
      "2024-01-21 17:41:10,222 - INFO - All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "2024-01-21 17:41:10,229 - INFO - All the weights of RobertaForTokenClassification were initialized from the model checkpoint at PlanTL-GOB-ES/roberta-large-bne-capitel-ner.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "2024-01-21 17:41:10,244 - INFO - Cargando datos de oraciones desde Excel...\n",
      "2024-01-21 17:41:11,715 - INFO - Realizando NER con RoBERTa...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando: 0/2208 oraciones (0.00%)\n",
      "Procesando: 220/2208 oraciones (9.96%)\n",
      "Procesando: 440/2208 oraciones (19.93%)\n",
      "Procesando: 660/2208 oraciones (29.89%)\n",
      "Procesando: 880/2208 oraciones (39.86%)\n",
      "Procesando: 1100/2208 oraciones (49.82%)\n",
      "Procesando: 1320/2208 oraciones (59.78%)\n",
      "Procesando: 1540/2208 oraciones (69.75%)\n",
      "Procesando: 1760/2208 oraciones (79.71%)\n",
      "\n",
      "Entidad 'Decreto de Urgencia N�� 017' no encontrada en el texto.\n",
      "Contexto: efectos del COVID - 19 [ENTIDAD] mediante el Decreto de Urgencia\n",
      "Oración: No obstante, lo anterior, mediante el Decreto de Urgencia Nº 017-2022 se han destinado 96.8 millones de soles para atender las demandas de las ollas comunes y enfrentar el incremento de los precios producto del contexto internacional y de los efectos del COVID-19.\n",
      "Omitiendo la adición de la entidad 'Decreto de Urgencia N�� 017' ya que no se encontró en el texto.\n",
      "Procesando: 1980/2208 oraciones (89.67%)\n",
      "Procesando: 2200/2208 oraciones (99.64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 17:55:26,256 - INFO - Agrupando entidades similares...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TFM_Project\\PLN_Project\\venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "2024-01-21 17:55:26,729 - INFO - Guardando datos de entidades RoBERTa en Excel...\n",
      "2024-01-21 17:55:27,102 - INFO - DataFrame guardado en d:\\TFM_Project\\PLN_Project\\data\\xlsx\\roberta_entity.xlsx\n"
     ]
    }
   ],
   "source": [
    "# notebooks/05_RoBERTa.ipynb\n",
    "# Importar las bibliotecas necesarias\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración del entorno del notebook\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils.file_utils import read_excel_file, save_data_to_excel\n",
    "from utils.advanced_ner_utils import (\n",
    "    perform_ner_with_roberta,\n",
    "    process_sentences_and_extract_entities,\n",
    ")\n",
    "from config import XLSX_DIRECTORY\n",
    "from utils.ner_utils import cluster_entities\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "# Carga de modelos y tokenizers\n",
    "roberta_model_name = \"PlanTL-GOB-ES/roberta-large-bne-capitel-ner\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(roberta_model_name)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Cargar los datos de las oraciones desde el archivo Excel\n",
    "    logging.info(\"Cargando datos de oraciones desde Excel...\")\n",
    "    sentence_data = read_excel_file(os.path.join(XLSX_DIRECTORY, \"sentence.xlsx\"))\n",
    "    # sentence_data = read_excel_file(os.path.join(XLSX_DIRECTORY, \"sentence_2.xlsx\"))\n",
    "\n",
    "    # Realizar NER con RoBERTa\n",
    "    logging.info(\"Realizando NER con RoBERTa...\")\n",
    "    roberta_entities_df = process_sentences_and_extract_entities(\n",
    "        sentence_data,\n",
    "        roberta_tokenizer,\n",
    "        roberta_model,\n",
    "        perform_ner_with_roberta,\n",
    "        \"roberta\",\n",
    "    )\n",
    "\n",
    "    # Convertir DataFrame de entidades a lista de diccionarios para el clustering\n",
    "    roberta_entities_df = roberta_entities_df.to_dict(\"records\")\n",
    "\n",
    "    # Opcional: Agrupar entidades similares con K-means\n",
    "    logging.info(\"Agrupando entidades similares...\")\n",
    "    roberta_entities_df = cluster_entities(roberta_entities_df, num_clusters=10)\n",
    "\n",
    "    # Guardar los datos de entidades RoBERTa en un nuevo archivo Excel\n",
    "    logging.info(\"Guardando datos de entidades RoBERTa en Excel...\")\n",
    "    save_data_to_excel(roberta_entities_df, XLSX_DIRECTORY, \"roberta_entity.xlsx\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice: 0, Token: 0, Decoded: , Label: O\n",
      "Índice: 1, Token: 7688, Decoded:  Hemos, Label: O\n",
      "Índice: 2, Token: 3997, Decoded:  llegado, Label: O\n",
      "Índice: 3, Token: 350, Decoded:  con, Label: O\n",
      "Índice: 4, Token: 344, Decoded:  el, Label: O\n",
      "Índice: 5, Token: 1614, Decoded:  Estado, Label: S-ORG\n",
      "Índice: 6, Token: 342, Decoded:  y, Label: O\n",
      "Entidad reconstruida (RoBERTa): Estado\n",
      "Índice: 7, Token: 534, Decoded:  sus, Label: O\n",
      "Índice: 8, Token: 1495, Decoded:  servicios, Label: O\n",
      "Índice: 9, Token: 27313, Decoded:  articul, Label: O\n",
      "Índice: 10, Token: 483, Decoded: ados, Label: O\n",
      "Índice: 11, Token: 320, Decoded:  a, Label: O\n",
      "Índice: 12, Token: 365, Decoded:  los, Label: O\n",
      "Índice: 13, Token: 14266, Decoded:  extremos, Label: O\n",
      "Índice: 14, Token: 313, Decoded:  de, Label: O\n",
      "Índice: 15, Token: 332, Decoded:  la, Label: O\n",
      "Índice: 16, Token: 11550, Decoded:  Amazon, Label: S-LOC\n",
      "Índice: 17, Token: 404, Decoded: ía, Label: E-LOC\n",
      "Índice: 18, Token: 379, Decoded:  A, Label: O\n",
      "Entidad reconstruida (RoBERTa): Amazonía\n",
      "Índice: 19, Token: 4631, Decoded:  comunidades, Label: O\n",
      "Índice: 20, Token: 469, Decoded:  como, Label: O\n",
      "Índice: 21, Token: 7532, Decoded:  Monte, Label: S-LOC\n",
      "Índice: 22, Token: 879, Decoded: ton, Label: E-LOC\n",
      "Índice: 23, Token: 127, Decoded: i, Label: E-LOC\n",
      "Índice: 24, Token: 66, Decoded: ,, Label: O\n",
      "Entidad reconstruida (RoBERTa): Montetoni\n",
      "Índice: 25, Token: 334, Decoded:  en, Label: O\n",
      "Índice: 26, Token: 344, Decoded:  el, Label: O\n",
      "Índice: 27, Token: 367, Decoded:  C, Label: S-LOC\n",
      "Índice: 28, Token: 43000, Decoded: usco, Label: E-LOC\n",
      "Índice: 29, Token: 378, Decoded:  o, Label: O\n",
      "Entidad reconstruida (RoBERTa): Cusco\n",
      "Índice: 30, Token: 34308, Decoded:  Pam, Label: B-LOC\n",
      "Índice: 31, Token: 441, Decoded: pa, Label: I-LOC\n",
      "Índice: 32, Token: 522, Decoded:  En, Label: E-LOC\n",
      "Índice: 33, Token: 138, Decoded: t, Label: I-LOC\n",
      "Índice: 34, Token: 1287, Decoded: sa, Label: E-LOC\n",
      "Índice: 35, Token: 66, Decoded: ,, Label: O\n",
      "Entidad reconstruida (RoBERTa): Pampa Entsa\n",
      "Índice: 36, Token: 334, Decoded:  en, Label: O\n",
      "Índice: 37, Token: 39858, Decoded:  Amazonas, Label: S-LOC\n",
      "Índice: 38, Token: 66, Decoded: ,, Label: O\n",
      "Entidad reconstruida (RoBERTa): Amazonas\n",
      "Índice: 39, Token: 400, Decoded:  para, Label: O\n",
      "Índice: 40, Token: 6699, Decoded:  acercar, Label: O\n",
      "Índice: 41, Token: 365, Decoded:  los, Label: O\n",
      "Índice: 42, Token: 1495, Decoded:  servicios, Label: O\n",
      "Índice: 43, Token: 8436, Decoded:  básicos, Label: O\n",
      "Índice: 44, Token: 342, Decoded:  y, Label: O\n",
      "Índice: 45, Token: 365, Decoded:  los, Label: O\n",
      "Índice: 46, Token: 3669, Decoded:  programas, Label: O\n",
      "Índice: 47, Token: 1970, Decoded:  sociales, Label: O\n",
      "Índice: 48, Token: 371, Decoded:  del, Label: O\n",
      "Índice: 49, Token: 1614, Decoded:  Estado, Label: S-ORG\n",
      "Índice: 50, Token: 68, Decoded: ., Label: O\n",
      "Entidad reconstruida (RoBERTa): Estado\n",
      "Índice: 51, Token: 2, Decoded: , Label: O\n",
      "\n",
      "Entidades extraídas:\n",
      "Estado\n",
      "Amazonía\n",
      "Montetoni\n",
      "Cusco\n",
      "Pampa Entsa\n",
      "Amazonas\n",
      "Estado\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Carga de modelos y tokenizers\n",
    "roberta_model_name = \"PlanTL-GOB-ES/roberta-large-bne-capitel-ner\"\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(roberta_model_name)\n",
    "\n",
    "def rebuild_entity_from_subtokens_roberta(subtokens, tokenizer):\n",
    "    entity = \"\"\n",
    "    for i, token in enumerate(subtokens):\n",
    "        decoded_token = tokenizer.decode([token], skip_special_tokens=True)\n",
    "        if i == 0:\n",
    "            entity = decoded_token\n",
    "        elif decoded_token.startswith(\"Ġ\"):\n",
    "            entity += \" \" + decoded_token[1:]\n",
    "        else:\n",
    "            entity += decoded_token\n",
    "\n",
    "    entity = entity.replace(\" - \", \"-\").replace(\" — \", \"—\").strip()\n",
    "    print(f\"Entidad reconstruida (RoBERTa): {entity}\")\n",
    "    return entity\n",
    "\n",
    "def perform_ner_with_roberta(text, tokenizer, model):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text, return_tensors=\"pt\", truncation=True, max_length=512\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    entities = []\n",
    "    current_entity_tokens = []\n",
    "\n",
    "    for idx, pred in enumerate(outputs.logits[0]):\n",
    "        label = model.config.id2label[pred.argmax().item()]\n",
    "        token = tokens.input_ids[0][idx]\n",
    "        token_str = tokenizer.decode([token], skip_special_tokens=True)\n",
    "        print(f\"Índice: {idx}, Token: {token}, Decoded: {token_str}, Label: {label}\")\n",
    "\n",
    "        if label.startswith(\"B-\") or label.startswith(\"S-\"):\n",
    "            if current_entity_tokens:\n",
    "                entity = rebuild_entity_from_subtokens_roberta(current_entity_tokens, tokenizer)\n",
    "                entities.append(entity)\n",
    "            current_entity_tokens = [token]\n",
    "        elif label.startswith(\"I-\") or label.startswith(\"E-\"):\n",
    "            current_entity_tokens.append(token)\n",
    "        else:\n",
    "            if current_entity_tokens:\n",
    "                entity = rebuild_entity_from_subtokens_roberta(current_entity_tokens, tokenizer)\n",
    "                entities.append(entity)\n",
    "                current_entity_tokens = []\n",
    "\n",
    "    if current_entity_tokens:\n",
    "        entity = rebuild_entity_from_subtokens_roberta(current_entity_tokens, tokenizer)\n",
    "        entities.append(entity)\n",
    "\n",
    "    return entities\n",
    "\n",
    "\n",
    "# Oración de entrada\n",
    "input_sentence = \"Hemos llegado con el Estado y sus servicios articulados a los extremos de la Amazonía A comunidades como Montetoni, en el Cusco o Pampa Entsa, en Amazonas, para acercar los servicios básicos y los programas sociales del Estado.\"\n",
    "extracted_entities = perform_ner_with_roberta(input_sentence, roberta_tokenizer, roberta_model)\n",
    "\n",
    "print(\"\\nEntidades extraídas:\")\n",
    "for entity in extracted_entities:\n",
    "    print(entity)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
